{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled162.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b34a1e44d8ec4d9bb25de98dba0f3e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a949c18f8d5740ba93c7364e77a52363",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_52a4258299d94362b54565ed58da08ec",
              "IPY_MODEL_c13695fbbd004012ab35844fa0456fb9"
            ]
          }
        },
        "a949c18f8d5740ba93c7364e77a52363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52a4258299d94362b54565ed58da08ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8b9a8259e9949f3bf84c4a47ac2f11b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96eb62a4f7e94889bcc8e18f1e405372"
          }
        },
        "c13695fbbd004012ab35844fa0456fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ef1eae1991e4c338098edb10eeff92b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/? [00:00&lt;00:00,  3.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ac03f331d0a42b0840c07e6facc9793"
          }
        },
        "d8b9a8259e9949f3bf84c4a47ac2f11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96eb62a4f7e94889bcc8e18f1e405372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ef1eae1991e4c338098edb10eeff92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ac03f331d0a42b0840c07e6facc9793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a948416e86314317840f304908cf4dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2eead50929d44994ad2b4b5b30ed0234",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbb21b31ca2c416886fa49bbd1a520f1",
              "IPY_MODEL_e9b3f4990e244c708bf774a93f46b957"
            ]
          }
        },
        "2eead50929d44994ad2b4b5b30ed0234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbb21b31ca2c416886fa49bbd1a520f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5221759a433d475b8cf7141ffdf3b1ac",
            "_dom_classes": [],
            "description": " 11%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 73,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6cb5b1f4c81496f8b65b9fb256f6b90"
          }
        },
        "e9b3f4990e244c708bf774a93f46b957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4a73d4f81dd847269fde5ab98b47250c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 73/649 [00:15&lt;02:05,  4.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40057da0152a4b1ab2227d067c09606b"
          }
        },
        "5221759a433d475b8cf7141ffdf3b1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6cb5b1f4c81496f8b65b9fb256f6b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a73d4f81dd847269fde5ab98b47250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40057da0152a4b1ab2227d067c09606b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/MS-Thesis-Phase3/blob/master/Models/Boroujerdi/Boroujerdi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkQPQqY6s8-v",
        "colab_type": "text"
      },
      "source": [
        "#In the name of God"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SudnGM-6qcaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce244e9c-a676-4d1f-8db8-67ff1c20e0e1"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "  function ClickConnect(){\n",
        "    console.log(\"Working\"); \n",
        "    document.querySelector(\"colab-connect-button\").click() \n",
        "  }\n",
        "  var connect_timer = setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "  function ClickConnect(){\n",
              "    console.log(\"Working\"); \n",
              "    document.querySelector(\"colab-connect-button\").click() \n",
              "  }\n",
              "  var connect_timer = setInterval(ClickConnect,60000)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTGb1dOrs48Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7995097e-d47b-4069-8bc8-24c3746bbf67"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 22 16:13:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyf240b5tD82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "316517b5-5dcd-4e6a-8a17-471b070473c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcbfW-tZtpLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "eb15bd57-9812-4e20-eddf-13fc7b640050"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675kB 4.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 19.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 63.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 50.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eweO40_dtxZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import AutoModel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X2E9kZ5tJRA",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eddoohNmtKug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = '/content/drive/My Drive/Thesis/phase-3/hkr_train.csv'\n",
        "valid_file =  '/content/drive/My Drive/Thesis/phase-3/hkr_valid.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgCX3Tv8tqPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "dec_tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-4_H-128_A-2')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu21rVUPuIvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"My dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, frac=1, split_rate=1, max_len=512, sort=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "        \"\"\"\n",
        "        self.dialogues = pd.read_csv(csv_file)\n",
        "        self.dialogues.dropna(inplace=True)\n",
        "        \n",
        "        self.dialogues = self.dialogues[self.dialogues.index % split_rate == 0]\n",
        "\n",
        "        self.dialogues = self.dialogues.sample(frac=frac)\n",
        "        \n",
        "        s = self.dialogues['response'].apply(dec_tokenizer.encode).apply(len).sort_values().index\n",
        "        self.dialogues = self.dialogues.reindex(s)\n",
        "\n",
        "        self.dialogues.dropna(inplace=True)\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    @staticmethod\n",
        "    def truncuate_join_pair_sentence(sentence1, sentence2, max_len=510):\n",
        "\n",
        "        \"\"\"\n",
        "        truncuate sentence one from head and sentence two from tail\n",
        "        Args:\n",
        "            sentence1 (string): first sentence\n",
        "            sentence2 (string): seconde sentence\n",
        "        \"\"\"\n",
        "        temp1 = enc_tokenizer.encode(sentence1,add_special_tokens=False)\n",
        "        temp2 = enc_tokenizer.encode(sentence2,add_special_tokens=False)\n",
        "        ### two above line may cause warning but no problem because we've handle them below\n",
        "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "        seq_1 = temp1\n",
        "        seq_2 = temp2\n",
        "        num_tokens_to_remove = len(temp1) + len(temp2) + 3 - max_len\n",
        "        if num_tokens_to_remove > 0 :\n",
        "            seq_1, seq_2, _ = enc_tokenizer.truncate_sequences(temp1[::-1],temp2, num_tokens_to_remove=num_tokens_to_remove)\n",
        "            seq_1.reverse()\n",
        "        result_list = [enc_tokenizer.cls_token_id]+seq_1+[enc_tokenizer.sep_token_id]+seq_2+[enc_tokenizer.sep_token_id]\n",
        "        token_type_ids = [0] * (len(seq_1) + 2) + [1] * (len(seq_2) + 1)\n",
        "        return result_list, token_type_ids\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "        \n",
        "        history = self.dialogues.iloc[idx].history\n",
        "        knowledge = self.dialogues.iloc[idx].knowledge\n",
        "        response = self.dialogues.iloc[idx].response\n",
        "\n",
        "\n",
        "        input_pair, input_pair_segments = MyDataset.truncuate_join_pair_sentence(history, knowledge, self.max_len)\n",
        "                \n",
        "\n",
        "        input_pair = torch.LongTensor(input_pair)\n",
        "\n",
        "        input_pair_segments = torch.LongTensor(input_pair_segments)\n",
        "\n",
        "        response_tensor = torch.LongTensor(dec_tokenizer.encode(response))\n",
        "\n",
        "        sample = {'input_pair': input_pair,\n",
        "                  'input_pair_segments': input_pair_segments,\n",
        "                  'response': response_tensor}\n",
        "\n",
        "        return sample\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0jkglqFwFQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2c1a93b0-26ea-4778-fb84-9d79966b8d5f"
      },
      "source": [
        "train_dataset = MyDataset(train_file, max_len=128)\n",
        "valid_dataset = MyDataset(valid_file, max_len=510)\n",
        "print(len(train_dataset))\n",
        "print(len(valid_dataset))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41489\n",
            "4458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VasXIkuLwHnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ce2bcfc6-526f-48c5-a13e-d78857189ac4"
      },
      "source": [
        "enc_tokenizer.decode(train_dataset[399]['input_pair'])\n",
        "#dec_tokenizer.decode(train_dataset[399]['response'])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] vancouver grizzlies [SEP] vancouver grizzlies were a canadian professional basketball team [SEP] i didn't even know canada had a basketball team! are they any good? [SEP] the vancouver grizzlies were a canadian professional basketball team based in vancouver, british columbia. [SEP]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Z5ZeAT2oii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "b34a1e44d8ec4d9bb25de98dba0f3e7b",
            "a949c18f8d5740ba93c7364e77a52363",
            "52a4258299d94362b54565ed58da08ec",
            "c13695fbbd004012ab35844fa0456fb9",
            "d8b9a8259e9949f3bf84c4a47ac2f11b",
            "96eb62a4f7e94889bcc8e18f1e405372",
            "0ef1eae1991e4c338098edb10eeff92b",
            "1ac03f331d0a42b0840c07e6facc9793"
          ]
        },
        "outputId": "3b365598-11a2-4632-f473-ccf6adb8c319"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "  len_batch = len(batch)\n",
        "\n",
        "  \n",
        "  max_len_input_pair = max([len(data['input_pair']) for data in batch])\n",
        "\n",
        "  max_len_response = max([len(data['response']) for data in batch])\n",
        "  \n",
        "  padding_ind = 0 ## for bert is 0 DON'T THINK BAD IT IS NOT REFACTORING !!!!!!\n",
        "  result_input_pair = torch.zeros(len_batch, max_len_input_pair)\n",
        "  result_input_pair_segments = torch.zeros(len_batch, max_len_input_pair)\n",
        "  result_response = torch.zeros(len_batch, max_len_response)\n",
        "\n",
        "  for i, data in enumerate(batch):\n",
        "    p1 = len(data['input_pair'])\n",
        "    result_input_pair[i, :p1] = data['input_pair']\n",
        "\n",
        "    p3 = len(data['input_pair_segments'])\n",
        "    result_input_pair_segments[i, :p3] = data['input_pair_segments']\n",
        "\n",
        "    p4 = len(data['response'])\n",
        "    result_response[i, :p4] = data['response']\n",
        "\n",
        "  return result_input_pair.long(), result_input_pair_segments.long(), result_response.long()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                             shuffle=False, collate_fn=my_collate_fn,\n",
        "                                           num_workers=1)\n",
        "\n",
        "#valid_sampler = torch.utils.data.SequentialSampler(valid_dataset)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64,\n",
        "                                             shuffle=False, collate_fn=my_collate_fn, num_workers=1)\n",
        "\n",
        "i = 0 \n",
        "for batch_idx, batch  in tqdm(enumerate(train_loader)):\n",
        "  pair_batch, segment_batch, response_batch = batch\n",
        "  print(pair_batch.shape)\n",
        "  print(segment_batch.shape)\n",
        "  print(response_batch.shape)\n",
        "  print(\"****\")\n",
        "  i += 1 \n",
        "  if(i==2):\n",
        "    break\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b34a1e44d8ec4d9bb25de98dba0f3e7b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 128])\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64, 6])\n",
            "****\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64, 7])\n",
            "****\n",
            "649\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXB7YVVgDyPU",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfGvoJMiEicR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizer\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.seq2seq = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "        'google/bert_uncased_L-2_H-128_A-2', 'google/bert_uncased_L-4_H-128_A-2')\n",
        "    \n",
        "    # for p in self.seq2seq.encoder.embeddings.parameters():\n",
        "    #   p.requires_grad = False\n",
        "    \n",
        "    # for p in self.seq2seq.decoder.bert.embeddings.parameters():\n",
        "    #   p.requires_grad = False\n",
        "\n",
        "  def forward(self, encoder_input, segments_tensors, decoder_input):\n",
        "    '''\n",
        "    encoder_input = [batch_size, enc_len]\n",
        "    segments_tensors = [batch_size, enc_len]\n",
        "    decoder_input = [batch_size, dec_len]\n",
        "    '''\n",
        "    kwargs = {'token_type_ids':segments_tensors}\n",
        "    outputs = self.seq2seq(input_ids=encoder_input, decoder_input_ids=decoder_input, **kwargs)[0]\n",
        "    return outputs\n",
        "  \n",
        "  def generate(self, encoder_input, segments_tensors):\n",
        "    ### encoder_input = [len] in int format\n",
        "    ### segment_tensors = [len]\n",
        "    encoder_input = encoder_input.unsqueeze(0)\n",
        "    segments_tensors = segments_tensors.unsqueeze(0)\n",
        "    kwargs = {'token_type_ids':segments_tensors}\n",
        "    generated = model.seq2seq.generate(encoder_input, decoder_start_token_id=101,\n",
        "                                       num_beams=32,\n",
        "                                       eos_token_id=102, ## [SEP] = 102\n",
        "                                       num_return_sequences=16,\n",
        "                                       **kwargs) ## [CLS] = 101\n",
        "    #### generated = [1, len]\n",
        "    return generated"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxsRPOHFGrBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e420ba45-a4b2-4405-def6-dc182650a630"
      },
      "source": [
        "dev = torch.device('cuda')\n",
        "model = Model().to(dev)\n",
        "\n",
        "# x = torch.LongTensor(200, 40).random_(1,1000).to(dev)\n",
        "# print(model(x).shape)\n",
        "\n",
        "\n",
        "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9480890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG3zXd-CAWSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "a18f4e52-db68-4dd5-a0ed-dccdc362fba0"
      },
      "source": [
        "x = torch.LongTensor(40).random_(120,1000).to(dev)\n",
        "y = torch.LongTensor(40).random_(0,2).to(dev)\n",
        "model.generate(x,y)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005,\n",
              "         2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005],\n",
              "        [ 101, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012,\n",
              "         1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012],\n",
              "        [ 101, 1996, 1996, 3588, 3588, 3588, 3588, 3588, 3588, 3588, 3588, 3588,\n",
              "         3588, 3588, 3588, 3588, 3588, 3588, 3588, 3588],\n",
              "        [ 101, 1996, 1996, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060,\n",
              "         2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060],\n",
              "        [ 101, 1996, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060,\n",
              "         2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060],\n",
              "        [ 101, 2055, 2055, 2055, 2055, 2055, 2055, 2055, 2055, 2055, 2055, 2055,\n",
              "         2055, 2055, 2055, 2055, 2055, 2055, 2055, 2055],\n",
              "        [ 101, 2058, 2058, 2058, 2058, 2058, 2058, 2058, 2058, 2058, 2058, 2058,\n",
              "         2058, 2058, 2058, 2058, 2058, 2058, 2058, 2058],\n",
              "        [ 101, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,\n",
              "         2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013],\n",
              "        [ 101, 2013, 2013, 2059, 2059, 2059, 2059, 2059, 2059, 2059, 2059, 2059,\n",
              "         2059, 2059, 2059, 2059, 2059, 2059, 2059, 2059],\n",
              "        [ 101, 1996, 1996, 6638, 6638, 6638, 6638, 6638, 6638, 6638, 6638, 6638,\n",
              "         6638, 6638, 6638, 6638, 6638, 6638, 6638, 6638],\n",
              "        [ 101, 2296, 2296, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060,\n",
              "         2060, 2060, 2060, 2060, 2060, 2060, 2060, 2060],\n",
              "        [ 101, 1996, 1996, 2111, 3588, 3588, 3588, 3588, 3588, 3588, 3588, 3588,\n",
              "         3588, 3588, 3588, 3588, 3588, 3588, 3588, 3588],\n",
              "        [ 101, 2105, 2105, 2105, 2105, 2105, 2105, 2105, 2105, 2105, 2105, 2105,\n",
              "         2105, 2105, 2105, 2105, 2105, 2105, 2105, 2105],\n",
              "        [ 101, 2013, 2013, 2013, 2059, 2059, 2059, 2059, 2059, 2059, 2059, 2059,\n",
              "         2059, 2059, 2059, 2059, 2059, 2059, 2059, 2059],\n",
              "        [ 101, 2004, 2004, 2574, 2574, 2574, 2574, 2574, 2574, 2574, 2574, 2574,\n",
              "         2574, 2574, 2574, 2574, 2574, 2574, 2574, 2574],\n",
              "        [ 101, 1996, 1996, 2364, 2364, 2364, 2364, 2364, 2364, 2364, 2364, 2364,\n",
              "         2364, 2364, 2364, 2364, 2364, 2364, 2364, 2364]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb5uvuCKL0cj",
        "colab_type": "text"
      },
      "source": [
        "#Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCzpzbgZN6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        return 1e-4\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxB7zLBDZQXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer2 = NoamOpt(128, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "optimizer = NoamOpt(128, 1, 1000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyeZLHUjZSTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "19b89a8f-daf2-4067-88bd-1121db53661c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.plot(np.arange(1, 20*649), [optimizer.rate(i) for i in range(1, 20*649)], color='blue')\n",
        "plt.plot(np.arange(1, 20*649), [optimizer2.rate(i) for i in range(1, 20*649)], color='green')\n",
        "plt.legend([\"128:2000\", \"512:8000\", \"256:4000\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f18e7050f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAhOZdxHJMoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eace864e-990d-4fa6-a8d9-fa7ed8b07633"
      },
      "source": [
        "print(\"Maximum learning rate is:\",max([optimizer.rate(i) for i in range(1, 20*649)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum learning rate is: 0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NNbXiU3xqj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3149838f-6164-4635-cc65-2bd2d6a29fee"
      },
      "source": [
        "optimizer.rate(20000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWALhfgJsZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d1c2e7d-d363-46c2-9db2-a86938d2ea95"
      },
      "source": [
        "print(\"Peak step is:\",max(enumerate([optimizer.rate(i) for i in range(1, 20*649)]), key=lambda x: x[1])[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peak step is: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufFe-Q62ZaFW",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-t7PADEZcZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn\n",
        "\n",
        "def mahdi_loss(model_output, true_trg, **kwargs):\n",
        "  '''\n",
        "  model_output: [batch, len, hidden]\n",
        "  true_trg: [batch, len]\n",
        "  '''\n",
        "  model_output = model_output[:,:-1,:]\n",
        "  true_trg = true_trg[:,1:]\n",
        "\n",
        "  if 'easy_training' in kwargs:\n",
        "    limit_last_tokens = kwargs['easy_training']\n",
        "    model_output = model_output[:,-limit_last_tokens:,:]\n",
        "    true_trg = true_trg[:,-limit_last_tokens:]\n",
        "\n",
        "  batch_len = model_output.shape[0]\n",
        "  snt_len = model_output.shape[1]\n",
        "  hidden_size = model_output.shape[2]\n",
        "\n",
        "  model_output = model_output.reshape(-1, hidden_size)\n",
        "  true_trg = true_trg.reshape(-1)\n",
        "\n",
        "  loss_mod = nn.CrossEntropyLoss(ignore_index=0)## PAD = 0\n",
        "  loss = loss_mod(model_output, true_trg)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmuTGJMJbR9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(batch_idx, batch):\n",
        "  pair_batch, segment_batch, response_batch = batch\n",
        "  pair_batch = pair_batch.to(dev)\n",
        "  segment_batch = segment_batch.to(dev)\n",
        "  response_batch = response_batch.to(dev)\n",
        "  model_output = model(pair_batch, segment_batch, response_batch)\n",
        "  #kwargs = {'easy_training':4}\n",
        "  loss = mahdi_loss(model_output, response_batch)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  del pair_batch\n",
        "  del segment_batch\n",
        "  del response_batch\n",
        "  return loss.item()\n",
        "\n",
        "def valid_step(batch_idx, batch):\n",
        "  with torch.no_grad():\n",
        "    pair_batch, segment_batch, response_batch = batch\n",
        "    pair_batch = pair_batch.to(dev)\n",
        "    segment_batch = segment_batch.to(dev)\n",
        "    response_batch = response_batch.to(dev)\n",
        "    model_output = model(pair_batch, segment_batch, response_batch)\n",
        "    loss = mahdi_loss(model_output, response_batch)\n",
        "    del pair_batch\n",
        "    del segment_batch\n",
        "    del response_batch\n",
        "    return loss.item()\n",
        "\n",
        "def valid_loop(valid_loader):\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "  for batch_idx, batch in tqdm(enumerate(valid_loader),  total=len(valid_loader)):\n",
        "    total_loss += valid_step(batch_idx, batch)\n",
        "  valid_inference(valid_loader)\n",
        "  model.train()\n",
        "  return total_loss / len(valid_loader)\n",
        "\n",
        "def valid_inference(valid_loader):\n",
        "  loader = enumerate(valid_loader)\n",
        "  next(loader)\n",
        "  next(loader)\n",
        "  idx, batch = next(loader)\n",
        "  pair_batch, segment_batch, response_batch = batch\n",
        "  pair_batch = pair_batch.to(dev)[0:8] ## Size batch is 8\n",
        "  segment_batch = segment_batch.to(dev)[0:8] ## Size batch is 8\n",
        "  response_batch = response_batch.to(dev)[0:8] ## Size batch is 8\n",
        "  model_output = model.generate(pair_batch, segment_batch)\n",
        "  print(\"pair is: \",tokenizer.decode(pair_batch[0]))\n",
        "  print(\"response is: \",tokenizer.decode(response_batch[0]))\n",
        "  print(\"model says: \",tokenizer.decode(model_output[0]))\n",
        "  print(\"---\"*3)\n",
        "  print(\"pair is: \",tokenizer.decode(pair_batch[0]))\n",
        "  print(\"response is: \",tokenizer.decode(response_batch[7]))\n",
        "  print(\"model says: \",tokenizer.decode(model_output[7]))\n",
        "  print(\"**** ****\"*5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pxjS0PQfKU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_learning = True\n",
        "if new_learning:\n",
        "  # optimizer = NoamOpt(128, 1, 2000,\n",
        "  #           torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "  model_dir = \"/content/drive/My Drive/Thesis/phase-3/Models/Boroujerdi/\"\n",
        "  step = 0\n",
        "  log_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZD1hD7rfNFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## if continue learning:\n",
        "#!wget -q https://github.com/mmsamiei/MS-Thesis-Phase2/raw/master/Models/hashemi_16000steps.model\n",
        "model_dir = \"/content/drive/My Drive/Thesis/phase-3/Models/Boroujerdi\"\n",
        "checkpoint = torch.load(model_dir+'boroujerdi_25000steps.model')\n",
        "step = checkpoint['log_list'][-1]['step']\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "optimizer._step = step\n",
        "log_list = checkpoint['log_list']\n",
        "new_learning = False\n",
        "print(step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHv6tC4YfZI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a948416e86314317840f304908cf4dc4",
            "2eead50929d44994ad2b4b5b30ed0234",
            "cbb21b31ca2c416886fa49bbd1a520f1",
            "e9b3f4990e244c708bf774a93f46b957",
            "5221759a433d475b8cf7141ffdf3b1ac",
            "d6cb5b1f4c81496f8b65b9fb256f6b90",
            "4a73d4f81dd847269fde5ab98b47250c",
            "40057da0152a4b1ab2227d067c09606b"
          ]
        },
        "outputId": "0cc3f463-8fcc-4a94-bf4f-43cb249d17c4"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "MAX_STEP = 20000\n",
        "STEP_SAVE = 1000\n",
        "STEP_CHECK = 1000\n",
        "step_num = step + 1\n",
        "log_list = log_list ### Check if new learning or not\n",
        "print(step_num)\n",
        "while step_num <= MAX_STEP:\n",
        "  model.train()\n",
        "  for batch_idx, batch in tqdm(enumerate(iter(train_loader)), total=len(train_loader)):\n",
        "    step_loss = train_step(batch_idx, batch)\n",
        "    log = {'step':step_num, 'train_loss':step_loss}\n",
        "\n",
        "    if(step_num % STEP_CHECK == 0):\n",
        "      valid_error = valid_loop(valid_loader)\n",
        "      print(\"valid Loss rate: {} at step {}\".format(valid_error, step_num))  \n",
        "      log['valid_loss'] = valid_error\n",
        "\n",
        "    log_list.append(log)\n",
        "\n",
        "    if(step_num % STEP_SAVE == 0):\n",
        "      torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'log_list': log_list,\n",
        "            'optimizer_state_dict': optimizer.optimizer.state_dict()\n",
        "            }, model_dir+'boroujerdi_{}steps.model'.format(step_num))\n",
        "    step_num += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a948416e86314317840f304908cf4dc4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=649.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001YzGTpEiV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4449703-2e1c-4c1e-ffd8-c1e7edc68f43"
      },
      "source": [
        "log_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'step': 1, 'train_loss': 14.16185474395752},\n",
              " {'step': 2, 'train_loss': 10.588570594787598},\n",
              " {'step': 3, 'train_loss': 8.754494667053223},\n",
              " {'step': 4, 'train_loss': 7.626872539520264},\n",
              " {'step': 5, 'train_loss': 7.459188938140869},\n",
              " {'step': 6, 'train_loss': 7.165883541107178},\n",
              " {'step': 7, 'train_loss': 6.705699443817139},\n",
              " {'step': 8, 'train_loss': 6.861814022064209},\n",
              " {'step': 9, 'train_loss': 7.119411945343018},\n",
              " {'step': 10, 'train_loss': 6.74100923538208},\n",
              " {'step': 11, 'train_loss': 6.516302108764648},\n",
              " {'step': 12, 'train_loss': 6.39729642868042},\n",
              " {'step': 13, 'train_loss': 6.507824420928955},\n",
              " {'step': 14, 'train_loss': 6.552713394165039},\n",
              " {'step': 15, 'train_loss': 6.616140842437744},\n",
              " {'step': 16, 'train_loss': 6.452396392822266},\n",
              " {'step': 17, 'train_loss': 6.47140645980835},\n",
              " {'step': 18, 'train_loss': 6.4972028732299805},\n",
              " {'step': 19, 'train_loss': 6.039199352264404},\n",
              " {'step': 20, 'train_loss': 6.292985439300537},\n",
              " {'step': 21, 'train_loss': 6.300108432769775},\n",
              " {'step': 22, 'train_loss': 6.189670562744141},\n",
              " {'step': 23, 'train_loss': 6.246770858764648},\n",
              " {'step': 24, 'train_loss': 6.452057838439941},\n",
              " {'step': 25, 'train_loss': 6.280635356903076},\n",
              " {'step': 26, 'train_loss': 6.273300647735596},\n",
              " {'step': 27, 'train_loss': 6.056609153747559},\n",
              " {'step': 28, 'train_loss': 6.026507377624512},\n",
              " {'step': 29, 'train_loss': 6.062320709228516},\n",
              " {'step': 30, 'train_loss': 5.975796222686768},\n",
              " {'step': 31, 'train_loss': 6.063614368438721},\n",
              " {'step': 32, 'train_loss': 6.074891090393066},\n",
              " {'step': 33, 'train_loss': 5.941574573516846},\n",
              " {'step': 34, 'train_loss': 5.894112586975098},\n",
              " {'step': 35, 'train_loss': 5.716279983520508},\n",
              " {'step': 36, 'train_loss': 5.833044528961182},\n",
              " {'step': 37, 'train_loss': 6.0303449630737305},\n",
              " {'step': 38, 'train_loss': 5.830294609069824},\n",
              " {'step': 39, 'train_loss': 6.028175354003906},\n",
              " {'step': 40, 'train_loss': 5.833225727081299},\n",
              " {'step': 41, 'train_loss': 5.704662799835205},\n",
              " {'step': 42, 'train_loss': 5.571741104125977},\n",
              " {'step': 43, 'train_loss': 5.689540386199951},\n",
              " {'step': 44, 'train_loss': 5.688337326049805},\n",
              " {'step': 45, 'train_loss': 5.639897346496582},\n",
              " {'step': 46, 'train_loss': 5.584630966186523},\n",
              " {'step': 47, 'train_loss': 5.806941509246826},\n",
              " {'step': 48, 'train_loss': 5.767697334289551},\n",
              " {'step': 49, 'train_loss': 5.917086124420166},\n",
              " {'step': 50, 'train_loss': 5.6431355476379395},\n",
              " {'step': 51, 'train_loss': 5.661414623260498},\n",
              " {'step': 52, 'train_loss': 5.869422912597656},\n",
              " {'step': 53, 'train_loss': 5.681264877319336},\n",
              " {'step': 54, 'train_loss': 5.396480083465576},\n",
              " {'step': 55, 'train_loss': 5.6130242347717285},\n",
              " {'step': 56, 'train_loss': 5.7876811027526855},\n",
              " {'step': 57, 'train_loss': 5.636516094207764},\n",
              " {'step': 58, 'train_loss': 5.453927993774414},\n",
              " {'step': 59, 'train_loss': 5.456034183502197},\n",
              " {'step': 60, 'train_loss': 5.442613124847412},\n",
              " {'step': 61, 'train_loss': 5.690744400024414},\n",
              " {'step': 62, 'train_loss': 5.448334217071533},\n",
              " {'step': 63, 'train_loss': 5.471738338470459},\n",
              " {'step': 64, 'train_loss': 5.292912006378174},\n",
              " {'step': 65, 'train_loss': 5.428622245788574},\n",
              " {'step': 66, 'train_loss': 5.616171836853027},\n",
              " {'step': 67, 'train_loss': 5.620097637176514},\n",
              " {'step': 68, 'train_loss': 5.393664836883545},\n",
              " {'step': 69, 'train_loss': 5.50708532333374},\n",
              " {'step': 70, 'train_loss': 5.605390548706055},\n",
              " {'step': 71, 'train_loss': 5.5498738288879395},\n",
              " {'step': 72, 'train_loss': 5.412203311920166},\n",
              " {'step': 73, 'train_loss': 5.582536697387695},\n",
              " {'step': 74, 'train_loss': 5.443815231323242},\n",
              " {'step': 75, 'train_loss': 5.685203552246094},\n",
              " {'step': 76, 'train_loss': 5.477849006652832},\n",
              " {'step': 77, 'train_loss': 5.542849063873291},\n",
              " {'step': 78, 'train_loss': 5.440784931182861},\n",
              " {'step': 79, 'train_loss': 5.087185859680176},\n",
              " {'step': 80, 'train_loss': 5.338245868682861},\n",
              " {'step': 81, 'train_loss': 5.356476306915283},\n",
              " {'step': 82, 'train_loss': 5.22061824798584},\n",
              " {'step': 83, 'train_loss': 5.371486186981201},\n",
              " {'step': 84, 'train_loss': 5.524362564086914},\n",
              " {'step': 85, 'train_loss': 5.548765659332275},\n",
              " {'step': 86, 'train_loss': 5.422755241394043},\n",
              " {'step': 87, 'train_loss': 5.400096416473389},\n",
              " {'step': 88, 'train_loss': 5.31420373916626},\n",
              " {'step': 89, 'train_loss': 5.216891765594482},\n",
              " {'step': 90, 'train_loss': 5.26539945602417},\n",
              " {'step': 91, 'train_loss': 5.467857837677002},\n",
              " {'step': 92, 'train_loss': 5.467442035675049},\n",
              " {'step': 93, 'train_loss': 5.221518516540527},\n",
              " {'step': 94, 'train_loss': 5.332440376281738},\n",
              " {'step': 95, 'train_loss': 5.321277618408203},\n",
              " {'step': 96, 'train_loss': 5.029416561126709},\n",
              " {'step': 97, 'train_loss': 5.127167224884033},\n",
              " {'step': 98, 'train_loss': 5.405561923980713},\n",
              " {'step': 99, 'train_loss': 5.287512302398682},\n",
              " {'step': 100,\n",
              "  'train_loss': 5.290463447570801,\n",
              "  'valid_loss': 5.823209537778582},\n",
              " {'step': 101, 'train_loss': 5.238887310028076},\n",
              " {'step': 102, 'train_loss': 5.2714009284973145},\n",
              " {'step': 103, 'train_loss': 5.212008953094482},\n",
              " {'step': 104, 'train_loss': 5.323022365570068},\n",
              " {'step': 105, 'train_loss': 5.193583965301514},\n",
              " {'step': 106, 'train_loss': 5.4864277839660645},\n",
              " {'step': 107, 'train_loss': 5.376293659210205},\n",
              " {'step': 108, 'train_loss': 5.0853681564331055},\n",
              " {'step': 109, 'train_loss': 5.334158897399902},\n",
              " {'step': 110, 'train_loss': 5.293378829956055},\n",
              " {'step': 111, 'train_loss': 5.36976432800293},\n",
              " {'step': 112, 'train_loss': 5.273850440979004},\n",
              " {'step': 113, 'train_loss': 5.235997200012207},\n",
              " {'step': 114, 'train_loss': 5.153770923614502},\n",
              " {'step': 115, 'train_loss': 5.2209649085998535},\n",
              " {'step': 116, 'train_loss': 5.247550964355469},\n",
              " {'step': 117, 'train_loss': 5.221922874450684},\n",
              " {'step': 118, 'train_loss': 4.9924235343933105},\n",
              " {'step': 119, 'train_loss': 5.212976455688477},\n",
              " {'step': 120, 'train_loss': 5.02761173248291},\n",
              " {'step': 121, 'train_loss': 5.12086296081543},\n",
              " {'step': 122, 'train_loss': 5.04570198059082},\n",
              " {'step': 123, 'train_loss': 4.8148579597473145},\n",
              " {'step': 124, 'train_loss': 4.896971702575684},\n",
              " {'step': 125, 'train_loss': 5.224151134490967},\n",
              " {'step': 126, 'train_loss': 5.175400733947754},\n",
              " {'step': 127, 'train_loss': 5.05117130279541},\n",
              " {'step': 128, 'train_loss': 5.091390609741211},\n",
              " {'step': 129, 'train_loss': 4.925715446472168},\n",
              " {'step': 130, 'train_loss': 5.087489604949951},\n",
              " {'step': 131, 'train_loss': 5.2539777755737305},\n",
              " {'step': 132, 'train_loss': 5.175881862640381},\n",
              " {'step': 133, 'train_loss': 5.140183925628662},\n",
              " {'step': 134, 'train_loss': 5.064090728759766},\n",
              " {'step': 135, 'train_loss': 5.028682708740234},\n",
              " {'step': 136, 'train_loss': 5.051665782928467},\n",
              " {'step': 137, 'train_loss': 5.214972019195557},\n",
              " {'step': 138, 'train_loss': 5.138067245483398},\n",
              " {'step': 139, 'train_loss': 5.205108642578125},\n",
              " {'step': 140, 'train_loss': 5.162613868713379},\n",
              " {'step': 141, 'train_loss': 5.01164436340332},\n",
              " {'step': 142, 'train_loss': 5.17160177230835},\n",
              " {'step': 143, 'train_loss': 5.042717933654785},\n",
              " {'step': 144, 'train_loss': 5.089645862579346},\n",
              " {'step': 145, 'train_loss': 4.944887161254883},\n",
              " {'step': 146, 'train_loss': 4.9781575202941895},\n",
              " {'step': 147, 'train_loss': 5.105286598205566},\n",
              " {'step': 148, 'train_loss': 4.964589595794678},\n",
              " {'step': 149, 'train_loss': 4.905205249786377},\n",
              " {'step': 150, 'train_loss': 5.1424784660339355},\n",
              " {'step': 151, 'train_loss': 5.141632556915283},\n",
              " {'step': 152, 'train_loss': 4.958339691162109},\n",
              " {'step': 153, 'train_loss': 4.853609561920166},\n",
              " {'step': 154, 'train_loss': 4.959444046020508},\n",
              " {'step': 155, 'train_loss': 4.936411380767822},\n",
              " {'step': 156, 'train_loss': 4.818442344665527},\n",
              " {'step': 157, 'train_loss': 5.241236209869385},\n",
              " {'step': 158, 'train_loss': 4.962329864501953},\n",
              " {'step': 159, 'train_loss': 5.026848316192627},\n",
              " {'step': 160, 'train_loss': 5.144440650939941},\n",
              " {'step': 161, 'train_loss': 5.078807830810547},\n",
              " {'step': 162, 'train_loss': 4.859644889831543},\n",
              " {'step': 163, 'train_loss': 5.066935062408447},\n",
              " {'step': 164, 'train_loss': 4.912227630615234},\n",
              " {'step': 165, 'train_loss': 5.003934383392334},\n",
              " {'step': 166, 'train_loss': 4.848867893218994},\n",
              " {'step': 167, 'train_loss': 4.987781047821045},\n",
              " {'step': 168, 'train_loss': 4.9935736656188965},\n",
              " {'step': 169, 'train_loss': 5.022176742553711},\n",
              " {'step': 170, 'train_loss': 4.874063491821289},\n",
              " {'step': 171, 'train_loss': 4.933993816375732},\n",
              " {'step': 172, 'train_loss': 5.039504051208496},\n",
              " {'step': 173, 'train_loss': 4.98220682144165},\n",
              " {'step': 174, 'train_loss': 4.854319095611572},\n",
              " {'step': 175, 'train_loss': 4.914142608642578},\n",
              " {'step': 176, 'train_loss': 4.790295600891113},\n",
              " {'step': 177, 'train_loss': 4.903184413909912},\n",
              " {'step': 178, 'train_loss': 4.875351428985596},\n",
              " {'step': 179, 'train_loss': 4.936644554138184},\n",
              " {'step': 180, 'train_loss': 4.965722560882568},\n",
              " {'step': 181, 'train_loss': 4.976412773132324},\n",
              " {'step': 182, 'train_loss': 4.751563549041748},\n",
              " {'step': 183, 'train_loss': 4.765707492828369},\n",
              " {'step': 184, 'train_loss': 4.995580673217773},\n",
              " {'step': 185, 'train_loss': 5.074025630950928},\n",
              " {'step': 186, 'train_loss': 4.972367763519287},\n",
              " {'step': 187, 'train_loss': 4.965402126312256},\n",
              " {'step': 188, 'train_loss': 5.014772415161133},\n",
              " {'step': 189, 'train_loss': 4.929333209991455},\n",
              " {'step': 190, 'train_loss': 4.932816028594971},\n",
              " {'step': 191, 'train_loss': 4.781198501586914},\n",
              " {'step': 192, 'train_loss': 4.934109210968018},\n",
              " {'step': 193, 'train_loss': 5.0265045166015625},\n",
              " {'step': 194, 'train_loss': 5.195173740386963},\n",
              " {'step': 195, 'train_loss': 4.8092360496521},\n",
              " {'step': 196, 'train_loss': 4.660788536071777},\n",
              " {'step': 197, 'train_loss': 4.813323020935059},\n",
              " {'step': 198, 'train_loss': 4.9514570236206055},\n",
              " {'step': 199, 'train_loss': 4.915849685668945},\n",
              " {'step': 200,\n",
              "  'train_loss': 5.026140213012695,\n",
              "  'valid_loss': 5.54295426096235},\n",
              " {'step': 201, 'train_loss': 4.932308197021484},\n",
              " {'step': 202, 'train_loss': 4.918108940124512},\n",
              " {'step': 203, 'train_loss': 4.829000473022461},\n",
              " {'step': 204, 'train_loss': 4.817355632781982},\n",
              " {'step': 205, 'train_loss': 4.866188049316406},\n",
              " {'step': 206, 'train_loss': 4.907449722290039},\n",
              " {'step': 207, 'train_loss': 4.80361795425415},\n",
              " {'step': 208, 'train_loss': 4.919430732727051},\n",
              " {'step': 209, 'train_loss': 4.81761360168457},\n",
              " {'step': 210, 'train_loss': 4.859610080718994},\n",
              " {'step': 211, 'train_loss': 4.875446796417236},\n",
              " {'step': 212, 'train_loss': 5.0287017822265625},\n",
              " {'step': 213, 'train_loss': 4.964881896972656},\n",
              " {'step': 214, 'train_loss': 4.9688568115234375},\n",
              " {'step': 215, 'train_loss': 4.816984176635742},\n",
              " {'step': 216, 'train_loss': 4.863687515258789},\n",
              " {'step': 217, 'train_loss': 4.777610778808594},\n",
              " {'step': 218, 'train_loss': 4.986746311187744},\n",
              " {'step': 219, 'train_loss': 4.845912933349609},\n",
              " {'step': 220, 'train_loss': 4.90897798538208},\n",
              " {'step': 221, 'train_loss': 4.796669006347656},\n",
              " {'step': 222, 'train_loss': 5.014331817626953},\n",
              " {'step': 223, 'train_loss': 4.895674705505371},\n",
              " {'step': 224, 'train_loss': 4.723274230957031},\n",
              " {'step': 225, 'train_loss': 4.893627643585205},\n",
              " {'step': 226, 'train_loss': 4.817057132720947},\n",
              " {'step': 227, 'train_loss': 4.849503993988037},\n",
              " {'step': 228, 'train_loss': 4.976110935211182},\n",
              " {'step': 229, 'train_loss': 4.800048351287842},\n",
              " {'step': 230, 'train_loss': 4.795438289642334},\n",
              " {'step': 231, 'train_loss': 4.9782209396362305},\n",
              " {'step': 232, 'train_loss': 4.831854343414307},\n",
              " {'step': 233, 'train_loss': 4.820845127105713},\n",
              " {'step': 234, 'train_loss': 4.777595520019531},\n",
              " {'step': 235, 'train_loss': 4.660221099853516},\n",
              " {'step': 236, 'train_loss': 4.528057098388672},\n",
              " {'step': 237, 'train_loss': 4.685755729675293},\n",
              " {'step': 238, 'train_loss': 4.779863357543945},\n",
              " {'step': 239, 'train_loss': 4.8113203048706055},\n",
              " {'step': 240, 'train_loss': 4.75559663772583},\n",
              " {'step': 241, 'train_loss': 4.9275126457214355},\n",
              " {'step': 242, 'train_loss': 4.928629398345947},\n",
              " {'step': 243, 'train_loss': 4.857390403747559},\n",
              " {'step': 244, 'train_loss': 4.843259334564209},\n",
              " {'step': 245, 'train_loss': 4.868549346923828},\n",
              " {'step': 246, 'train_loss': 5.059941291809082},\n",
              " {'step': 247, 'train_loss': 4.804190158843994},\n",
              " {'step': 248, 'train_loss': 4.7601165771484375},\n",
              " {'step': 249, 'train_loss': 4.896338939666748},\n",
              " {'step': 250, 'train_loss': 4.722661018371582},\n",
              " {'step': 251, 'train_loss': 4.96254825592041},\n",
              " {'step': 252, 'train_loss': 4.694850444793701},\n",
              " {'step': 253, 'train_loss': 4.8591628074646},\n",
              " {'step': 254, 'train_loss': 4.889159202575684},\n",
              " {'step': 255, 'train_loss': 4.760134696960449},\n",
              " {'step': 256, 'train_loss': 4.747522354125977},\n",
              " {'step': 257, 'train_loss': 4.929926872253418},\n",
              " {'step': 258, 'train_loss': 4.8690948486328125},\n",
              " {'step': 259, 'train_loss': 4.858828067779541},\n",
              " {'step': 260, 'train_loss': 4.7294721603393555},\n",
              " {'step': 261, 'train_loss': 4.787950038909912},\n",
              " {'step': 262, 'train_loss': 4.842779636383057},\n",
              " {'step': 263, 'train_loss': 4.808744430541992},\n",
              " {'step': 264, 'train_loss': 4.739518642425537},\n",
              " {'step': 265, 'train_loss': 4.833774089813232},\n",
              " {'step': 266, 'train_loss': 4.875141620635986},\n",
              " {'step': 267, 'train_loss': 4.914432525634766},\n",
              " {'step': 268, 'train_loss': 4.673774242401123},\n",
              " {'step': 269, 'train_loss': 4.784067153930664},\n",
              " {'step': 270, 'train_loss': 4.822796821594238},\n",
              " {'step': 271, 'train_loss': 4.6964111328125},\n",
              " {'step': 272, 'train_loss': 4.671392917633057},\n",
              " {'step': 273, 'train_loss': 4.795265197753906},\n",
              " {'step': 274, 'train_loss': 4.87824010848999},\n",
              " {'step': 275, 'train_loss': 5.004493236541748},\n",
              " {'step': 276, 'train_loss': 4.942490100860596},\n",
              " {'step': 277, 'train_loss': 4.821893215179443},\n",
              " {'step': 278, 'train_loss': 4.8194074630737305},\n",
              " {'step': 279, 'train_loss': 4.701443672180176},\n",
              " {'step': 280, 'train_loss': 4.704490661621094},\n",
              " {'step': 281, 'train_loss': 4.619851589202881},\n",
              " {'step': 282, 'train_loss': 4.598305702209473},\n",
              " {'step': 283, 'train_loss': 4.731791019439697},\n",
              " {'step': 284, 'train_loss': 4.95516300201416},\n",
              " {'step': 285, 'train_loss': 4.704682350158691},\n",
              " {'step': 286, 'train_loss': 4.67237663269043},\n",
              " {'step': 287, 'train_loss': 4.758463382720947},\n",
              " {'step': 288, 'train_loss': 4.7759928703308105},\n",
              " {'step': 289, 'train_loss': 4.697208404541016},\n",
              " {'step': 290, 'train_loss': 4.796821594238281},\n",
              " {'step': 291, 'train_loss': 4.762394428253174},\n",
              " {'step': 292, 'train_loss': 4.700649261474609},\n",
              " {'step': 293, 'train_loss': 4.694566249847412},\n",
              " {'step': 294, 'train_loss': 4.826957702636719},\n",
              " {'step': 295, 'train_loss': 4.593539714813232},\n",
              " {'step': 296, 'train_loss': 4.543375492095947},\n",
              " {'step': 297, 'train_loss': 4.61456298828125},\n",
              " {'step': 298, 'train_loss': 4.617681980133057},\n",
              " {'step': 299, 'train_loss': 4.830827236175537},\n",
              " {'step': 300,\n",
              "  'train_loss': 4.6938581466674805,\n",
              "  'valid_loss': 5.445670938491821},\n",
              " {'step': 301, 'train_loss': 4.844503402709961},\n",
              " {'step': 302, 'train_loss': 4.8401570320129395},\n",
              " {'step': 303, 'train_loss': 4.835399627685547},\n",
              " {'step': 304, 'train_loss': 4.814539432525635},\n",
              " {'step': 305, 'train_loss': 4.7751784324646},\n",
              " {'step': 306, 'train_loss': 4.893603801727295},\n",
              " {'step': 307, 'train_loss': 4.6666178703308105},\n",
              " {'step': 308, 'train_loss': 4.8155083656311035},\n",
              " {'step': 309, 'train_loss': 4.678257465362549},\n",
              " {'step': 310, 'train_loss': 4.709998607635498},\n",
              " {'step': 311, 'train_loss': 4.72310209274292},\n",
              " {'step': 312, 'train_loss': 4.833346843719482},\n",
              " {'step': 313, 'train_loss': 4.725427150726318},\n",
              " {'step': 314, 'train_loss': 4.6124749183654785},\n",
              " {'step': 315, 'train_loss': 4.763779163360596},\n",
              " {'step': 316, 'train_loss': 4.743270397186279},\n",
              " {'step': 317, 'train_loss': 4.588045120239258},\n",
              " {'step': 318, 'train_loss': 4.776163578033447},\n",
              " {'step': 319, 'train_loss': 4.7578935623168945},\n",
              " {'step': 320, 'train_loss': 4.740284442901611},\n",
              " {'step': 321, 'train_loss': 4.875303745269775},\n",
              " {'step': 322, 'train_loss': 4.781793594360352},\n",
              " {'step': 323, 'train_loss': 4.756076335906982},\n",
              " {'step': 324, 'train_loss': 4.640713691711426},\n",
              " {'step': 325, 'train_loss': 4.730323314666748},\n",
              " {'step': 326, 'train_loss': 4.779529571533203},\n",
              " {'step': 327, 'train_loss': 4.7625837326049805},\n",
              " {'step': 328, 'train_loss': 4.666973114013672},\n",
              " {'step': 329, 'train_loss': 4.727555751800537},\n",
              " {'step': 330, 'train_loss': 4.610171794891357},\n",
              " {'step': 331, 'train_loss': 4.745595932006836},\n",
              " {'step': 332, 'train_loss': 4.661873817443848},\n",
              " {'step': 333, 'train_loss': 4.904203414916992},\n",
              " {'step': 334, 'train_loss': 4.791934967041016},\n",
              " {'step': 335, 'train_loss': 4.835606575012207},\n",
              " {'step': 336, 'train_loss': 4.667689323425293},\n",
              " {'step': 337, 'train_loss': 4.656595230102539},\n",
              " {'step': 338, 'train_loss': 4.758028984069824},\n",
              " {'step': 339, 'train_loss': 4.83323860168457},\n",
              " {'step': 340, 'train_loss': 4.627089023590088},\n",
              " {'step': 341, 'train_loss': 4.733526229858398},\n",
              " {'step': 342, 'train_loss': 4.6215620040893555},\n",
              " {'step': 343, 'train_loss': 4.678311347961426},\n",
              " {'step': 344, 'train_loss': 4.756782531738281},\n",
              " {'step': 345, 'train_loss': 4.910965919494629},\n",
              " {'step': 346, 'train_loss': 4.663033485412598},\n",
              " {'step': 347, 'train_loss': 4.709228038787842},\n",
              " {'step': 348, 'train_loss': 4.798034191131592},\n",
              " {'step': 349, 'train_loss': 4.600996971130371},\n",
              " {'step': 350, 'train_loss': 4.628338813781738},\n",
              " {'step': 351, 'train_loss': 4.607990741729736},\n",
              " {'step': 352, 'train_loss': 4.672189712524414},\n",
              " {'step': 353, 'train_loss': 4.723597526550293},\n",
              " {'step': 354, 'train_loss': 4.804142475128174},\n",
              " {'step': 355, 'train_loss': 4.6813836097717285},\n",
              " {'step': 356, 'train_loss': 4.750008583068848},\n",
              " {'step': 357, 'train_loss': 4.729371547698975},\n",
              " {'step': 358, 'train_loss': 4.645566940307617},\n",
              " {'step': 359, 'train_loss': 4.731334686279297},\n",
              " {'step': 360, 'train_loss': 4.661136150360107},\n",
              " {'step': 361, 'train_loss': 4.674532890319824},\n",
              " {'step': 362, 'train_loss': 4.879196643829346},\n",
              " {'step': 363, 'train_loss': 4.634122371673584},\n",
              " {'step': 364, 'train_loss': 4.6652512550354},\n",
              " {'step': 365, 'train_loss': 4.932816982269287},\n",
              " {'step': 366, 'train_loss': 4.87034273147583},\n",
              " {'step': 367, 'train_loss': 4.662930965423584},\n",
              " {'step': 368, 'train_loss': 4.828505516052246},\n",
              " {'step': 369, 'train_loss': 4.670597553253174},\n",
              " {'step': 370, 'train_loss': 4.877627849578857},\n",
              " {'step': 371, 'train_loss': 4.792471408843994},\n",
              " {'step': 372, 'train_loss': 4.819291591644287},\n",
              " {'step': 373, 'train_loss': 4.760438919067383},\n",
              " {'step': 374, 'train_loss': 4.681023120880127},\n",
              " {'step': 375, 'train_loss': 4.764410018920898},\n",
              " {'step': 376, 'train_loss': 4.77895975112915},\n",
              " {'step': 377, 'train_loss': 4.850160121917725},\n",
              " {'step': 378, 'train_loss': 4.759493350982666},\n",
              " {'step': 379, 'train_loss': 4.639034271240234},\n",
              " {'step': 380, 'train_loss': 4.617595672607422},\n",
              " {'step': 381, 'train_loss': 4.633214473724365},\n",
              " {'step': 382, 'train_loss': 4.73839807510376},\n",
              " {'step': 383, 'train_loss': 4.716282367706299},\n",
              " {'step': 384, 'train_loss': 4.610538005828857},\n",
              " {'step': 385, 'train_loss': 4.623270511627197},\n",
              " {'step': 386, 'train_loss': 4.828407287597656},\n",
              " {'step': 387, 'train_loss': 4.797022819519043},\n",
              " {'step': 388, 'train_loss': 4.744808673858643},\n",
              " {'step': 389, 'train_loss': 4.501043319702148},\n",
              " {'step': 390, 'train_loss': 4.644697666168213},\n",
              " {'step': 391, 'train_loss': 4.620615482330322},\n",
              " {'step': 392, 'train_loss': 4.7034406661987305},\n",
              " {'step': 393, 'train_loss': 4.737051486968994},\n",
              " {'step': 394, 'train_loss': 4.779163837432861},\n",
              " {'step': 395, 'train_loss': 4.723454475402832},\n",
              " {'step': 396, 'train_loss': 4.63914680480957},\n",
              " {'step': 397, 'train_loss': 4.774782657623291},\n",
              " {'step': 398, 'train_loss': 4.786055088043213},\n",
              " {'step': 399, 'train_loss': 4.567605495452881},\n",
              " {'step': 400,\n",
              "  'train_loss': 4.6826653480529785,\n",
              "  'valid_loss': 5.346644060952323},\n",
              " {'step': 401, 'train_loss': 4.675041675567627},\n",
              " {'step': 402, 'train_loss': 4.673377513885498},\n",
              " {'step': 403, 'train_loss': 4.552521228790283},\n",
              " {'step': 404, 'train_loss': 4.606475830078125},\n",
              " {'step': 405, 'train_loss': 4.730044364929199},\n",
              " {'step': 406, 'train_loss': 4.654655933380127},\n",
              " {'step': 407, 'train_loss': 4.762310981750488},\n",
              " {'step': 408, 'train_loss': 4.566617012023926},\n",
              " {'step': 409, 'train_loss': 4.638118743896484},\n",
              " {'step': 410, 'train_loss': 4.743715763092041},\n",
              " {'step': 411, 'train_loss': 4.710366249084473},\n",
              " {'step': 412, 'train_loss': 4.77088737487793},\n",
              " {'step': 413, 'train_loss': 4.699418544769287},\n",
              " {'step': 414, 'train_loss': 4.7448506355285645},\n",
              " {'step': 415, 'train_loss': 4.707182884216309},\n",
              " {'step': 416, 'train_loss': 4.767192363739014},\n",
              " {'step': 417, 'train_loss': 4.77849006652832},\n",
              " {'step': 418, 'train_loss': 4.839099884033203},\n",
              " {'step': 419, 'train_loss': 4.701307773590088},\n",
              " {'step': 420, 'train_loss': 4.609561443328857},\n",
              " {'step': 421, 'train_loss': 4.629398822784424},\n",
              " {'step': 422, 'train_loss': 4.607202529907227},\n",
              " {'step': 423, 'train_loss': 4.78108024597168},\n",
              " {'step': 424, 'train_loss': 4.777617931365967},\n",
              " {'step': 425, 'train_loss': 4.722369194030762},\n",
              " {'step': 426, 'train_loss': 4.618376731872559},\n",
              " {'step': 427, 'train_loss': 4.800785541534424},\n",
              " {'step': 428, 'train_loss': 4.6971235275268555},\n",
              " {'step': 429, 'train_loss': 4.618885040283203},\n",
              " {'step': 430, 'train_loss': 4.743460655212402},\n",
              " {'step': 431, 'train_loss': 4.729734897613525},\n",
              " {'step': 432, 'train_loss': 4.6285624504089355},\n",
              " {'step': 433, 'train_loss': 4.685783863067627},\n",
              " {'step': 434, 'train_loss': 4.557346820831299},\n",
              " {'step': 435, 'train_loss': 4.887965202331543},\n",
              " {'step': 436, 'train_loss': 4.740845680236816},\n",
              " {'step': 437, 'train_loss': 4.690654754638672},\n",
              " {'step': 438, 'train_loss': 4.763461112976074},\n",
              " {'step': 439, 'train_loss': 4.6026105880737305},\n",
              " {'step': 440, 'train_loss': 4.816668510437012},\n",
              " {'step': 441, 'train_loss': 4.683244705200195},\n",
              " {'step': 442, 'train_loss': 4.73674201965332},\n",
              " {'step': 443, 'train_loss': 4.699542999267578},\n",
              " {'step': 444, 'train_loss': 4.735312461853027},\n",
              " {'step': 445, 'train_loss': 4.85668420791626},\n",
              " {'step': 446, 'train_loss': 4.636843204498291},\n",
              " {'step': 447, 'train_loss': 4.676257610321045},\n",
              " {'step': 448, 'train_loss': 4.723430156707764},\n",
              " {'step': 449, 'train_loss': 4.532830238342285},\n",
              " {'step': 450, 'train_loss': 4.618477821350098},\n",
              " {'step': 451, 'train_loss': 4.772864818572998},\n",
              " {'step': 452, 'train_loss': 4.573425769805908},\n",
              " {'step': 453, 'train_loss': 4.697084903717041},\n",
              " {'step': 454, 'train_loss': 4.692176818847656},\n",
              " {'step': 455, 'train_loss': 4.77533483505249},\n",
              " {'step': 456, 'train_loss': 4.544423580169678},\n",
              " {'step': 457, 'train_loss': 4.783784866333008},\n",
              " {'step': 458, 'train_loss': 4.724153995513916},\n",
              " {'step': 459, 'train_loss': 4.743695259094238},\n",
              " {'step': 460, 'train_loss': 4.765627384185791},\n",
              " {'step': 461, 'train_loss': 4.806800365447998},\n",
              " {'step': 462, 'train_loss': 4.9186601638793945},\n",
              " {'step': 463, 'train_loss': 4.693702697753906},\n",
              " {'step': 464, 'train_loss': 4.69635009765625},\n",
              " {'step': 465, 'train_loss': 4.7332611083984375},\n",
              " {'step': 466, 'train_loss': 4.677013874053955},\n",
              " {'step': 467, 'train_loss': 4.767759799957275},\n",
              " {'step': 468, 'train_loss': 4.970199108123779},\n",
              " {'step': 469, 'train_loss': 4.514068126678467},\n",
              " {'step': 470, 'train_loss': 4.69742488861084},\n",
              " {'step': 471, 'train_loss': 4.633244514465332},\n",
              " {'step': 472, 'train_loss': 4.631295680999756},\n",
              " {'step': 473, 'train_loss': 4.787206172943115},\n",
              " {'step': 474, 'train_loss': 4.724809169769287},\n",
              " {'step': 475, 'train_loss': 4.551987171173096},\n",
              " {'step': 476, 'train_loss': 4.729552268981934},\n",
              " {'step': 477, 'train_loss': 4.723694324493408},\n",
              " {'step': 478, 'train_loss': 4.9178361892700195},\n",
              " {'step': 479, 'train_loss': 4.810097694396973},\n",
              " {'step': 480, 'train_loss': 4.777570724487305},\n",
              " {'step': 481, 'train_loss': 4.60344934463501},\n",
              " {'step': 482, 'train_loss': 4.664989948272705},\n",
              " {'step': 483, 'train_loss': 4.656327247619629},\n",
              " {'step': 484, 'train_loss': 4.694092273712158},\n",
              " {'step': 485, 'train_loss': 4.655835151672363},\n",
              " {'step': 486, 'train_loss': 4.828627109527588},\n",
              " {'step': 487, 'train_loss': 4.658268451690674},\n",
              " {'step': 488, 'train_loss': 4.660094738006592},\n",
              " {'step': 489, 'train_loss': 4.562456130981445},\n",
              " {'step': 490, 'train_loss': 4.590194225311279},\n",
              " {'step': 491, 'train_loss': 4.624749660491943},\n",
              " {'step': 492, 'train_loss': 4.6636962890625},\n",
              " {'step': 493, 'train_loss': 4.637856960296631},\n",
              " {'step': 494, 'train_loss': 4.659600734710693},\n",
              " {'step': 495, 'train_loss': 4.724188804626465},\n",
              " {'step': 496, 'train_loss': 4.669137954711914},\n",
              " {'step': 497, 'train_loss': 4.66870641708374},\n",
              " {'step': 498, 'train_loss': 4.809542655944824},\n",
              " {'step': 499, 'train_loss': 4.607548713684082},\n",
              " {'step': 500,\n",
              "  'train_loss': 4.636635780334473,\n",
              "  'valid_loss': 5.275369800840106},\n",
              " {'step': 501, 'train_loss': 4.52524995803833},\n",
              " {'step': 502, 'train_loss': 4.737273216247559},\n",
              " {'step': 503, 'train_loss': 4.775355815887451},\n",
              " {'step': 504, 'train_loss': 4.8534440994262695},\n",
              " {'step': 505, 'train_loss': 4.763758182525635},\n",
              " {'step': 506, 'train_loss': 4.7248125076293945},\n",
              " {'step': 507, 'train_loss': 4.811273097991943},\n",
              " {'step': 508, 'train_loss': 4.729446887969971},\n",
              " {'step': 509, 'train_loss': 4.712453365325928},\n",
              " {'step': 510, 'train_loss': 4.755614280700684},\n",
              " {'step': 511, 'train_loss': 4.683717250823975},\n",
              " {'step': 512, 'train_loss': 4.692434787750244},\n",
              " {'step': 513, 'train_loss': 4.75631856918335},\n",
              " {'step': 514, 'train_loss': 4.684567928314209},\n",
              " {'step': 515, 'train_loss': 4.561679840087891},\n",
              " {'step': 516, 'train_loss': 4.88765287399292},\n",
              " {'step': 517, 'train_loss': 4.708027362823486},\n",
              " {'step': 518, 'train_loss': 4.745851039886475},\n",
              " {'step': 519, 'train_loss': 4.874701023101807},\n",
              " {'step': 520, 'train_loss': 4.913914680480957},\n",
              " {'step': 521, 'train_loss': 4.7921366691589355},\n",
              " {'step': 522, 'train_loss': 4.801284313201904},\n",
              " {'step': 523, 'train_loss': 4.7007155418396},\n",
              " {'step': 524, 'train_loss': 4.766933441162109},\n",
              " {'step': 525, 'train_loss': 4.820535182952881},\n",
              " {'step': 526, 'train_loss': 4.588505744934082},\n",
              " {'step': 527, 'train_loss': 4.689375400543213},\n",
              " {'step': 528, 'train_loss': 4.5768327713012695},\n",
              " {'step': 529, 'train_loss': 4.575439453125},\n",
              " {'step': 530, 'train_loss': 4.754021167755127},\n",
              " {'step': 531, 'train_loss': 4.823306560516357},\n",
              " {'step': 532, 'train_loss': 4.711146831512451},\n",
              " {'step': 533, 'train_loss': 4.746954917907715},\n",
              " {'step': 534, 'train_loss': 4.791675567626953},\n",
              " {'step': 535, 'train_loss': 4.699037075042725},\n",
              " {'step': 536, 'train_loss': 4.974743366241455},\n",
              " {'step': 537, 'train_loss': 4.6901631355285645},\n",
              " {'step': 538, 'train_loss': 4.581165313720703},\n",
              " {'step': 539, 'train_loss': 4.633076190948486},\n",
              " {'step': 540, 'train_loss': 4.651427268981934},\n",
              " {'step': 541, 'train_loss': 4.799322128295898},\n",
              " {'step': 542, 'train_loss': 4.665083408355713},\n",
              " {'step': 543, 'train_loss': 4.790781497955322},\n",
              " {'step': 544, 'train_loss': 4.832665920257568},\n",
              " {'step': 545, 'train_loss': 4.812819480895996},\n",
              " {'step': 546, 'train_loss': 4.63370943069458},\n",
              " {'step': 547, 'train_loss': 4.7549614906311035},\n",
              " {'step': 548, 'train_loss': 4.888620853424072},\n",
              " {'step': 549, 'train_loss': 4.775920391082764},\n",
              " {'step': 550, 'train_loss': 4.640279293060303},\n",
              " {'step': 551, 'train_loss': 4.687062740325928},\n",
              " {'step': 552, 'train_loss': 4.746585369110107},\n",
              " {'step': 553, 'train_loss': 4.63829231262207},\n",
              " {'step': 554, 'train_loss': 4.798140048980713},\n",
              " {'step': 555, 'train_loss': 4.670523166656494},\n",
              " {'step': 556, 'train_loss': 4.645259380340576},\n",
              " {'step': 557, 'train_loss': 4.872668743133545},\n",
              " {'step': 558, 'train_loss': 4.681727409362793},\n",
              " {'step': 559, 'train_loss': 4.782008647918701},\n",
              " {'step': 560, 'train_loss': 4.636221408843994},\n",
              " {'step': 561, 'train_loss': 4.775344371795654},\n",
              " {'step': 562, 'train_loss': 4.7664475440979},\n",
              " {'step': 563, 'train_loss': 4.753025531768799},\n",
              " {'step': 564, 'train_loss': 4.747917175292969},\n",
              " {'step': 565, 'train_loss': 4.708774566650391},\n",
              " {'step': 566, 'train_loss': 4.633457660675049},\n",
              " {'step': 567, 'train_loss': 4.66802453994751},\n",
              " {'step': 568, 'train_loss': 4.698133945465088},\n",
              " {'step': 569, 'train_loss': 4.73972749710083},\n",
              " {'step': 570, 'train_loss': 4.783381938934326},\n",
              " {'step': 571, 'train_loss': 4.695085048675537},\n",
              " {'step': 572, 'train_loss': 4.799773216247559},\n",
              " {'step': 573, 'train_loss': 4.707979679107666},\n",
              " {'step': 574, 'train_loss': 4.702939510345459},\n",
              " {'step': 575, 'train_loss': 4.735695838928223},\n",
              " {'step': 576, 'train_loss': 4.734930038452148},\n",
              " {'step': 577, 'train_loss': 4.649556636810303},\n",
              " {'step': 578, 'train_loss': 4.8852152824401855},\n",
              " {'step': 579, 'train_loss': 4.727743625640869},\n",
              " {'step': 580, 'train_loss': 4.747348308563232},\n",
              " {'step': 581, 'train_loss': 4.632375717163086},\n",
              " {'step': 582, 'train_loss': 4.591567039489746},\n",
              " {'step': 583, 'train_loss': 4.756657123565674},\n",
              " {'step': 584, 'train_loss': 4.710386276245117},\n",
              " {'step': 585, 'train_loss': 4.7458343505859375},\n",
              " {'step': 586, 'train_loss': 4.693596363067627},\n",
              " {'step': 587, 'train_loss': 4.782123565673828},\n",
              " {'step': 588, 'train_loss': 4.728944301605225},\n",
              " {'step': 589, 'train_loss': 4.767319202423096},\n",
              " {'step': 590, 'train_loss': 4.7579665184021},\n",
              " {'step': 591, 'train_loss': 4.652619361877441},\n",
              " {'step': 592, 'train_loss': 4.816257953643799},\n",
              " {'step': 593, 'train_loss': 4.6470770835876465},\n",
              " {'step': 594, 'train_loss': 4.93497896194458},\n",
              " {'step': 595, 'train_loss': 4.620888710021973},\n",
              " {'step': 596, 'train_loss': 4.869086742401123},\n",
              " {'step': 597, 'train_loss': 4.726615905761719},\n",
              " {'step': 598, 'train_loss': 4.882898330688477},\n",
              " {'step': 599, 'train_loss': 4.753476142883301},\n",
              " {'step': 600,\n",
              "  'train_loss': 4.749767780303955,\n",
              "  'valid_loss': 5.293262243270874},\n",
              " {'step': 601, 'train_loss': 4.835971355438232},\n",
              " {'step': 602, 'train_loss': 4.827794075012207},\n",
              " {'step': 603, 'train_loss': 4.913280487060547},\n",
              " {'step': 604, 'train_loss': 4.7321553230285645},\n",
              " {'step': 605, 'train_loss': 4.804868698120117},\n",
              " {'step': 606, 'train_loss': 4.86391019821167},\n",
              " {'step': 607, 'train_loss': 4.60609245300293},\n",
              " {'step': 608, 'train_loss': 4.795348167419434},\n",
              " {'step': 609, 'train_loss': 4.834147930145264},\n",
              " {'step': 610, 'train_loss': 4.80106258392334},\n",
              " {'step': 611, 'train_loss': 4.893710136413574},\n",
              " {'step': 612, 'train_loss': 4.779280662536621},\n",
              " {'step': 613, 'train_loss': 4.785711288452148},\n",
              " {'step': 614, 'train_loss': 4.698680400848389},\n",
              " {'step': 615, 'train_loss': 4.810117244720459},\n",
              " {'step': 616, 'train_loss': 4.642062187194824},\n",
              " {'step': 617, 'train_loss': 4.83765983581543},\n",
              " {'step': 618, 'train_loss': 4.827215671539307},\n",
              " {'step': 619, 'train_loss': 4.839496612548828},\n",
              " {'step': 620, 'train_loss': 4.8108744621276855},\n",
              " {'step': 621, 'train_loss': 4.915859699249268},\n",
              " {'step': 622, 'train_loss': 4.881525039672852},\n",
              " {'step': 623, 'train_loss': 4.7330427169799805},\n",
              " {'step': 624, 'train_loss': 4.744163513183594},\n",
              " {'step': 625, 'train_loss': 4.860647201538086},\n",
              " {'step': 626, 'train_loss': 4.84774112701416},\n",
              " {'step': 627, 'train_loss': 4.867400646209717},\n",
              " {'step': 628, 'train_loss': 4.956194877624512},\n",
              " {'step': 629, 'train_loss': 4.847441673278809},\n",
              " {'step': 630, 'train_loss': 4.820918560028076},\n",
              " {'step': 631, 'train_loss': 4.6578264236450195},\n",
              " {'step': 632, 'train_loss': 4.798579216003418},\n",
              " {'step': 633, 'train_loss': 4.738925457000732},\n",
              " {'step': 634, 'train_loss': 4.897169589996338},\n",
              " {'step': 635, 'train_loss': 4.8653564453125},\n",
              " {'step': 636, 'train_loss': 4.974111080169678},\n",
              " {'step': 637, 'train_loss': 4.985699653625488},\n",
              " {'step': 638, 'train_loss': 4.814745903015137},\n",
              " {'step': 639, 'train_loss': 4.8235859870910645},\n",
              " {'step': 640, 'train_loss': 4.8500285148620605},\n",
              " {'step': 641, 'train_loss': 4.945553302764893},\n",
              " {'step': 642, 'train_loss': 4.889034271240234},\n",
              " {'step': 643, 'train_loss': 4.834896087646484},\n",
              " {'step': 644, 'train_loss': 4.986661434173584},\n",
              " {'step': 645, 'train_loss': 4.990405559539795},\n",
              " {'step': 646, 'train_loss': 5.175078868865967},\n",
              " {'step': 647, 'train_loss': 5.1001362800598145},\n",
              " {'step': 648, 'train_loss': 5.2336249351501465},\n",
              " {'step': 649, 'train_loss': 5.14789342880249},\n",
              " {'step': 650, 'train_loss': 9.109679222106934},\n",
              " {'step': 651, 'train_loss': 7.004627227783203},\n",
              " {'step': 652, 'train_loss': 5.84831428527832},\n",
              " {'step': 653, 'train_loss': 4.887032985687256},\n",
              " {'step': 654, 'train_loss': 4.397239685058594},\n",
              " {'step': 655, 'train_loss': 5.090844631195068},\n",
              " {'step': 656, 'train_loss': 4.93619966506958},\n",
              " {'step': 657, 'train_loss': 4.705620765686035},\n",
              " {'step': 658, 'train_loss': 4.981796741485596},\n",
              " {'step': 659, 'train_loss': 4.723294258117676},\n",
              " {'step': 660, 'train_loss': 4.614151954650879},\n",
              " {'step': 661, 'train_loss': 4.352842330932617},\n",
              " {'step': 662, 'train_loss': 4.525052547454834},\n",
              " {'step': 663, 'train_loss': 4.619803428649902},\n",
              " {'step': 664, 'train_loss': 4.788598537445068},\n",
              " {'step': 665, 'train_loss': 4.444990158081055},\n",
              " {'step': 666, 'train_loss': 4.529276371002197},\n",
              " {'step': 667, 'train_loss': 4.626638412475586},\n",
              " {'step': 668, 'train_loss': 4.334490776062012},\n",
              " {'step': 669, 'train_loss': 4.6343560218811035},\n",
              " {'step': 670, 'train_loss': 4.433805465698242},\n",
              " {'step': 671, 'train_loss': 4.498913288116455},\n",
              " {'step': 672, 'train_loss': 4.587981224060059},\n",
              " {'step': 673, 'train_loss': 4.668491363525391},\n",
              " {'step': 674, 'train_loss': 4.577359676361084},\n",
              " {'step': 675, 'train_loss': 4.510356426239014},\n",
              " {'step': 676, 'train_loss': 4.4756669998168945},\n",
              " {'step': 677, 'train_loss': 4.285432815551758},\n",
              " {'step': 678, 'train_loss': 4.373899459838867},\n",
              " {'step': 679, 'train_loss': 4.371013164520264},\n",
              " {'step': 680, 'train_loss': 4.463187217712402},\n",
              " {'step': 681, 'train_loss': 4.528497219085693},\n",
              " {'step': 682, 'train_loss': 4.391573905944824},\n",
              " {'step': 683, 'train_loss': 4.39181661605835},\n",
              " {'step': 684, 'train_loss': 4.298435688018799},\n",
              " {'step': 685, 'train_loss': 4.488317966461182},\n",
              " {'step': 686, 'train_loss': 4.546215534210205},\n",
              " {'step': 687, 'train_loss': 4.377556324005127},\n",
              " {'step': 688, 'train_loss': 4.766483306884766},\n",
              " {'step': 689, 'train_loss': 4.543011665344238},\n",
              " {'step': 690, 'train_loss': 4.271510124206543},\n",
              " {'step': 691, 'train_loss': 4.209789752960205},\n",
              " {'step': 692, 'train_loss': 4.364612579345703},\n",
              " {'step': 693, 'train_loss': 4.350624084472656},\n",
              " {'step': 694, 'train_loss': 4.225891590118408},\n",
              " {'step': 695, 'train_loss': 4.340835094451904},\n",
              " {'step': 696, 'train_loss': 4.404979228973389},\n",
              " {'step': 697, 'train_loss': 4.592503547668457},\n",
              " {'step': 698, 'train_loss': 4.656270503997803},\n",
              " {'step': 699, 'train_loss': 4.532431125640869},\n",
              " {'step': 700,\n",
              "  'train_loss': 4.434880256652832,\n",
              "  'valid_loss': 4.826259810583932},\n",
              " {'step': 701, 'train_loss': 4.623404502868652},\n",
              " {'step': 702, 'train_loss': 4.579867839813232},\n",
              " {'step': 703, 'train_loss': 4.1964945793151855},\n",
              " {'step': 704, 'train_loss': 4.292031764984131},\n",
              " {'step': 705, 'train_loss': 4.572146415710449},\n",
              " {'step': 706, 'train_loss': 4.564513206481934},\n",
              " {'step': 707, 'train_loss': 4.266706466674805},\n",
              " {'step': 708, 'train_loss': 4.331949710845947},\n",
              " {'step': 709, 'train_loss': 4.270078182220459},\n",
              " {'step': 710, 'train_loss': 4.5193562507629395},\n",
              " {'step': 711, 'train_loss': 4.3424506187438965},\n",
              " {'step': 712, 'train_loss': 4.260861396789551},\n",
              " {'step': 713, 'train_loss': 4.256899356842041},\n",
              " {'step': 714, 'train_loss': 4.439578056335449},\n",
              " {'step': 715, 'train_loss': 4.573756694793701},\n",
              " {'step': 716, 'train_loss': 4.494505882263184},\n",
              " {'step': 717, 'train_loss': 4.320614337921143},\n",
              " {'step': 718, 'train_loss': 4.442336082458496},\n",
              " {'step': 719, 'train_loss': 4.599659442901611},\n",
              " {'step': 720, 'train_loss': 4.533692359924316},\n",
              " {'step': 721, 'train_loss': 4.3355255126953125},\n",
              " {'step': 722, 'train_loss': 4.407537460327148},\n",
              " {'step': 723, 'train_loss': 4.417603015899658},\n",
              " {'step': 724, 'train_loss': 4.680809020996094},\n",
              " {'step': 725, 'train_loss': 4.45203161239624},\n",
              " {'step': 726, 'train_loss': 4.526670455932617},\n",
              " {'step': 727, 'train_loss': 4.474836349487305},\n",
              " {'step': 728, 'train_loss': 4.009507656097412},\n",
              " {'step': 729, 'train_loss': 4.4461140632629395},\n",
              " {'step': 730, 'train_loss': 4.382490634918213},\n",
              " {'step': 731, 'train_loss': 4.332301616668701},\n",
              " {'step': 732, 'train_loss': 4.416817665100098},\n",
              " {'step': 733, 'train_loss': 4.6189775466918945},\n",
              " {'step': 734, 'train_loss': 4.516356945037842},\n",
              " {'step': 735, 'train_loss': 4.4319376945495605},\n",
              " {'step': 736, 'train_loss': 4.516080379486084},\n",
              " {'step': 737, 'train_loss': 4.428802967071533},\n",
              " {'step': 738, 'train_loss': 4.346526145935059},\n",
              " {'step': 739, 'train_loss': 4.391401767730713},\n",
              " {'step': 740, 'train_loss': 4.450401782989502},\n",
              " {'step': 741, 'train_loss': 4.5067315101623535},\n",
              " {'step': 742, 'train_loss': 4.314320087432861},\n",
              " {'step': 743, 'train_loss': 4.505476951599121},\n",
              " {'step': 744, 'train_loss': 4.434414386749268},\n",
              " {'step': 745, 'train_loss': 4.150450229644775},\n",
              " {'step': 746, 'train_loss': 4.3159074783325195},\n",
              " {'step': 747, 'train_loss': 4.459837913513184},\n",
              " {'step': 748, 'train_loss': 4.426249027252197},\n",
              " {'step': 749, 'train_loss': 4.482034206390381},\n",
              " {'step': 750, 'train_loss': 4.392688274383545},\n",
              " {'step': 751, 'train_loss': 4.453463077545166},\n",
              " {'step': 752, 'train_loss': 4.262679576873779},\n",
              " {'step': 753, 'train_loss': 4.572542667388916},\n",
              " {'step': 754, 'train_loss': 4.412543773651123},\n",
              " {'step': 755, 'train_loss': 4.653314113616943},\n",
              " {'step': 756, 'train_loss': 4.535012722015381},\n",
              " {'step': 757, 'train_loss': 4.246656894683838},\n",
              " {'step': 758, 'train_loss': 4.482542037963867},\n",
              " {'step': 759, 'train_loss': 4.4788994789123535},\n",
              " {'step': 760, 'train_loss': 4.5109968185424805},\n",
              " {'step': 761, 'train_loss': 4.3874969482421875},\n",
              " {'step': 762, 'train_loss': 4.447875499725342},\n",
              " {'step': 763, 'train_loss': 4.343556880950928},\n",
              " {'step': 764, 'train_loss': 4.399359703063965},\n",
              " {'step': 765, 'train_loss': 4.352089881896973},\n",
              " {'step': 766, 'train_loss': 4.392806053161621},\n",
              " {'step': 767, 'train_loss': 4.2448859214782715},\n",
              " {'step': 768, 'train_loss': 4.42279052734375},\n",
              " {'step': 769, 'train_loss': 4.307247638702393},\n",
              " {'step': 770, 'train_loss': 4.470232009887695},\n",
              " {'step': 771, 'train_loss': 4.235327243804932},\n",
              " {'step': 772, 'train_loss': 4.096709728240967},\n",
              " {'step': 773, 'train_loss': 4.106789588928223},\n",
              " {'step': 774, 'train_loss': 4.4796671867370605},\n",
              " {'step': 775, 'train_loss': 4.438159465789795},\n",
              " {'step': 776, 'train_loss': 4.368139743804932},\n",
              " {'step': 777, 'train_loss': 4.262160301208496},\n",
              " {'step': 778, 'train_loss': 4.22482442855835},\n",
              " {'step': 779, 'train_loss': 4.356376647949219},\n",
              " {'step': 780, 'train_loss': 4.5780768394470215},\n",
              " {'step': 781, 'train_loss': 4.504478454589844},\n",
              " {'step': 782, 'train_loss': 4.420700550079346},\n",
              " {'step': 783, 'train_loss': 4.3714280128479},\n",
              " {'step': 784, 'train_loss': 4.38604736328125},\n",
              " {'step': 785, 'train_loss': 4.332695484161377},\n",
              " {'step': 786, 'train_loss': 4.510317325592041},\n",
              " {'step': 787, 'train_loss': 4.487914085388184},\n",
              " {'step': 788, 'train_loss': 4.527728080749512},\n",
              " {'step': 789, 'train_loss': 4.524243354797363},\n",
              " {'step': 790, 'train_loss': 4.313713550567627},\n",
              " {'step': 791, 'train_loss': 4.475809097290039},\n",
              " {'step': 792, 'train_loss': 4.376941680908203},\n",
              " {'step': 793, 'train_loss': 4.299545764923096},\n",
              " {'step': 794, 'train_loss': 4.294064044952393},\n",
              " {'step': 795, 'train_loss': 4.217931747436523},\n",
              " {'step': 796, 'train_loss': 4.50429105758667},\n",
              " {'step': 797, 'train_loss': 4.262434482574463},\n",
              " {'step': 798, 'train_loss': 4.194103240966797},\n",
              " {'step': 799, 'train_loss': 4.512797832489014},\n",
              " {'step': 800,\n",
              "  'train_loss': 4.468797206878662,\n",
              "  'valid_loss': 5.106768996374948},\n",
              " {'step': 801, 'train_loss': 4.300169467926025},\n",
              " {'step': 802, 'train_loss': 4.2628655433654785},\n",
              " {'step': 803, 'train_loss': 4.268305778503418},\n",
              " {'step': 804, 'train_loss': 4.2831268310546875},\n",
              " {'step': 805, 'train_loss': 4.19259786605835},\n",
              " {'step': 806, 'train_loss': 4.5943193435668945},\n",
              " {'step': 807, 'train_loss': 4.362175941467285},\n",
              " {'step': 808, 'train_loss': 4.345180511474609},\n",
              " {'step': 809, 'train_loss': 4.629295825958252},\n",
              " {'step': 810, 'train_loss': 4.466188907623291},\n",
              " {'step': 811, 'train_loss': 4.216412544250488},\n",
              " {'step': 812, 'train_loss': 4.446636199951172},\n",
              " {'step': 813, 'train_loss': 4.268394470214844},\n",
              " {'step': 814, 'train_loss': 4.430345058441162},\n",
              " {'step': 815, 'train_loss': 4.201312065124512},\n",
              " {'step': 816, 'train_loss': 4.4182844161987305},\n",
              " {'step': 817, 'train_loss': 4.363307476043701},\n",
              " {'step': 818, 'train_loss': 4.528223514556885},\n",
              " {'step': 819, 'train_loss': 4.424612045288086},\n",
              " {'step': 820, 'train_loss': 4.3464035987854},\n",
              " {'step': 821, 'train_loss': 4.39862060546875},\n",
              " {'step': 822, 'train_loss': 4.323718547821045},\n",
              " {'step': 823, 'train_loss': 4.295719146728516},\n",
              " {'step': 824, 'train_loss': 4.230048656463623},\n",
              " {'step': 825, 'train_loss': 4.181216716766357},\n",
              " {'step': 826, 'train_loss': 4.22067403793335},\n",
              " {'step': 827, 'train_loss': 4.239110946655273},\n",
              " {'step': 828, 'train_loss': 4.446796417236328},\n",
              " {'step': 829, 'train_loss': 4.380545139312744},\n",
              " {'step': 830, 'train_loss': 4.363830089569092},\n",
              " {'step': 831, 'train_loss': 4.10322380065918},\n",
              " {'step': 832, 'train_loss': 4.134634494781494},\n",
              " {'step': 833, 'train_loss': 4.359652042388916},\n",
              " {'step': 834, 'train_loss': 4.520097732543945},\n",
              " {'step': 835, 'train_loss': 4.423554420471191},\n",
              " {'step': 836, 'train_loss': 4.384646415710449},\n",
              " {'step': 837, 'train_loss': 4.4849534034729},\n",
              " {'step': 838, 'train_loss': 4.301294326782227},\n",
              " {'step': 839, 'train_loss': 4.43770694732666},\n",
              " {'step': 840, 'train_loss': 4.2197418212890625},\n",
              " {'step': 841, 'train_loss': 4.322351932525635},\n",
              " {'step': 842, 'train_loss': 4.43754768371582},\n",
              " {'step': 843, 'train_loss': 4.690217018127441},\n",
              " {'step': 844, 'train_loss': 4.27587890625},\n",
              " {'step': 845, 'train_loss': 4.084646224975586},\n",
              " {'step': 846, 'train_loss': 4.245536804199219},\n",
              " {'step': 847, 'train_loss': 4.500897407531738},\n",
              " {'step': 848, 'train_loss': 4.308292865753174},\n",
              " {'step': 849, 'train_loss': 4.567070007324219},\n",
              " {'step': 850, 'train_loss': 4.391465663909912},\n",
              " {'step': 851, 'train_loss': 4.36925745010376},\n",
              " {'step': 852, 'train_loss': 4.266512870788574},\n",
              " {'step': 853, 'train_loss': 4.269679546356201},\n",
              " {'step': 854, 'train_loss': 4.365640640258789},\n",
              " {'step': 855, 'train_loss': 4.417253017425537},\n",
              " {'step': 856, 'train_loss': 4.285833358764648},\n",
              " {'step': 857, 'train_loss': 4.367757320404053},\n",
              " {'step': 858, 'train_loss': 4.302289962768555},\n",
              " {'step': 859, 'train_loss': 4.372890949249268},\n",
              " {'step': 860, 'train_loss': 4.383955955505371},\n",
              " {'step': 861, 'train_loss': 4.514763832092285},\n",
              " {'step': 862, 'train_loss': 4.472640514373779},\n",
              " {'step': 863, 'train_loss': 4.440080165863037},\n",
              " {'step': 864, 'train_loss': 4.4010515213012695},\n",
              " {'step': 865, 'train_loss': 4.367393970489502},\n",
              " {'step': 866, 'train_loss': 4.307084560394287},\n",
              " {'step': 867, 'train_loss': 4.468810081481934},\n",
              " {'step': 868, 'train_loss': 4.3804931640625},\n",
              " {'step': 869, 'train_loss': 4.408309459686279},\n",
              " {'step': 870, 'train_loss': 4.21518087387085},\n",
              " {'step': 871, 'train_loss': 4.538379669189453},\n",
              " {'step': 872, 'train_loss': 4.454352855682373},\n",
              " {'step': 873, 'train_loss': 4.313803195953369},\n",
              " {'step': 874, 'train_loss': 4.4067912101745605},\n",
              " {'step': 875, 'train_loss': 4.339052677154541},\n",
              " {'step': 876, 'train_loss': 4.388404369354248},\n",
              " {'step': 877, 'train_loss': 4.524081230163574},\n",
              " {'step': 878, 'train_loss': 4.329559803009033},\n",
              " {'step': 879, 'train_loss': 4.293783664703369},\n",
              " {'step': 880, 'train_loss': 4.496062278747559},\n",
              " {'step': 881, 'train_loss': 4.3510613441467285},\n",
              " {'step': 882, 'train_loss': 4.266261100769043},\n",
              " {'step': 883, 'train_loss': 4.302517414093018},\n",
              " {'step': 884, 'train_loss': 4.2572808265686035},\n",
              " {'step': 885, 'train_loss': 4.101836681365967},\n",
              " {'step': 886, 'train_loss': 4.217494964599609},\n",
              " {'step': 887, 'train_loss': 4.329551696777344},\n",
              " {'step': 888, 'train_loss': 4.329798698425293},\n",
              " {'step': 889, 'train_loss': 4.346057891845703},\n",
              " {'step': 890, 'train_loss': 4.512980937957764},\n",
              " {'step': 891, 'train_loss': 4.466795921325684},\n",
              " {'step': 892, 'train_loss': 4.348085880279541},\n",
              " {'step': 893, 'train_loss': 4.414681434631348},\n",
              " {'step': 894, 'train_loss': 4.402318000793457},\n",
              " {'step': 895, 'train_loss': 4.645906925201416},\n",
              " {'step': 896, 'train_loss': 4.347723484039307},\n",
              " {'step': 897, 'train_loss': 4.343084812164307},\n",
              " {'step': 898, 'train_loss': 4.420262336730957},\n",
              " {'step': 899, 'train_loss': 4.305126190185547},\n",
              " {'step': 900,\n",
              "  'train_loss': 4.568698406219482,\n",
              "  'valid_loss': 4.9592933859143935},\n",
              " {'step': 901, 'train_loss': 4.190789222717285},\n",
              " {'step': 902, 'train_loss': 4.409821033477783},\n",
              " {'step': 903, 'train_loss': 4.40513801574707},\n",
              " {'step': 904, 'train_loss': 4.3235650062561035},\n",
              " {'step': 905, 'train_loss': 4.282802581787109},\n",
              " {'step': 906, 'train_loss': 4.522090435028076},\n",
              " {'step': 907, 'train_loss': 4.474932670593262},\n",
              " {'step': 908, 'train_loss': 4.444377422332764},\n",
              " {'step': 909, 'train_loss': 4.347197532653809},\n",
              " {'step': 910, 'train_loss': 4.3920793533325195},\n",
              " {'step': 911, 'train_loss': 4.445748329162598},\n",
              " {'step': 912, 'train_loss': 4.385473728179932},\n",
              " {'step': 913, 'train_loss': 4.318730354309082},\n",
              " {'step': 914, 'train_loss': 4.432267665863037},\n",
              " {'step': 915, 'train_loss': 4.490477561950684},\n",
              " {'step': 916, 'train_loss': 4.43775749206543},\n",
              " {'step': 917, 'train_loss': 4.245631694793701},\n",
              " {'step': 918, 'train_loss': 4.3666582107543945},\n",
              " {'step': 919, 'train_loss': 4.435795783996582},\n",
              " {'step': 920, 'train_loss': 4.232016086578369},\n",
              " {'step': 921, 'train_loss': 4.3002424240112305},\n",
              " {'step': 922, 'train_loss': 4.4201459884643555},\n",
              " {'step': 923, 'train_loss': 4.419990539550781},\n",
              " {'step': 924, 'train_loss': 4.502818584442139},\n",
              " {'step': 925, 'train_loss': 4.481664657592773},\n",
              " {'step': 926, 'train_loss': 4.408269882202148},\n",
              " {'step': 927, 'train_loss': 4.366469383239746},\n",
              " {'step': 928, 'train_loss': 4.2590436935424805},\n",
              " {'step': 929, 'train_loss': 4.305606842041016},\n",
              " {'step': 930, 'train_loss': 4.242644309997559},\n",
              " {'step': 931, 'train_loss': 4.14793586730957},\n",
              " {'step': 932, 'train_loss': 4.27426815032959},\n",
              " {'step': 933, 'train_loss': 4.539551734924316},\n",
              " {'step': 934, 'train_loss': 4.329168796539307},\n",
              " {'step': 935, 'train_loss': 4.290035247802734},\n",
              " {'step': 936, 'train_loss': 4.310324192047119},\n",
              " {'step': 937, 'train_loss': 4.426572799682617},\n",
              " {'step': 938, 'train_loss': 4.236004829406738},\n",
              " {'step': 939, 'train_loss': 4.384681224822998},\n",
              " {'step': 940, 'train_loss': 4.323431968688965},\n",
              " {'step': 941, 'train_loss': 4.311143398284912},\n",
              " {'step': 942, 'train_loss': 4.3495659828186035},\n",
              " {'step': 943, 'train_loss': 4.449721813201904},\n",
              " {'step': 944, 'train_loss': 4.218267917633057},\n",
              " {'step': 945, 'train_loss': 4.173862934112549},\n",
              " {'step': 946, 'train_loss': 4.251789093017578},\n",
              " {'step': 947, 'train_loss': 4.232183456420898},\n",
              " {'step': 948, 'train_loss': 4.427674293518066},\n",
              " {'step': 949, 'train_loss': 4.241252422332764},\n",
              " {'step': 950, 'train_loss': 4.370894432067871},\n",
              " {'step': 951, 'train_loss': 4.503661632537842},\n",
              " {'step': 952, 'train_loss': 4.520747661590576},\n",
              " {'step': 953, 'train_loss': 4.464183330535889},\n",
              " {'step': 954, 'train_loss': 4.320326805114746},\n",
              " {'step': 955, 'train_loss': 4.507665634155273},\n",
              " {'step': 956, 'train_loss': 4.325619220733643},\n",
              " {'step': 957, 'train_loss': 4.421635627746582},\n",
              " {'step': 958, 'train_loss': 4.206228733062744},\n",
              " {'step': 959, 'train_loss': 4.279170513153076},\n",
              " {'step': 960, 'train_loss': 4.320297718048096},\n",
              " {'step': 961, 'train_loss': 4.431244373321533},\n",
              " {'step': 962, 'train_loss': 4.393827438354492},\n",
              " {'step': 963, 'train_loss': 4.209045886993408},\n",
              " {'step': 964, 'train_loss': 4.3530659675598145},\n",
              " {'step': 965, 'train_loss': 4.420654296875},\n",
              " {'step': 966, 'train_loss': 4.2473297119140625},\n",
              " {'step': 967, 'train_loss': 4.412833213806152},\n",
              " {'step': 968, 'train_loss': 4.388889312744141},\n",
              " {'step': 969, 'train_loss': 4.3958001136779785},\n",
              " {'step': 970, 'train_loss': 4.430582046508789},\n",
              " {'step': 971, 'train_loss': 4.3596649169921875},\n",
              " {'step': 972, 'train_loss': 4.407469272613525},\n",
              " {'step': 973, 'train_loss': 4.28360652923584},\n",
              " {'step': 974, 'train_loss': 4.298564434051514},\n",
              " {'step': 975, 'train_loss': 4.465524673461914},\n",
              " {'step': 976, 'train_loss': 4.465251445770264},\n",
              " {'step': 977, 'train_loss': 4.3864827156066895},\n",
              " {'step': 978, 'train_loss': 4.350946426391602},\n",
              " {'step': 979, 'train_loss': 4.290414333343506},\n",
              " {'step': 980, 'train_loss': 4.3616228103637695},\n",
              " {'step': 981, 'train_loss': 4.264698505401611},\n",
              " {'step': 982, 'train_loss': 4.600488662719727},\n",
              " {'step': 983, 'train_loss': 4.448400497436523},\n",
              " {'step': 984, 'train_loss': 4.434776782989502},\n",
              " {'step': 985, 'train_loss': 4.259401321411133},\n",
              " {'step': 986, 'train_loss': 4.403656482696533},\n",
              " {'step': 987, 'train_loss': 4.3763532638549805},\n",
              " {'step': 988, 'train_loss': 4.477755546569824},\n",
              " {'step': 989, 'train_loss': 4.260756015777588},\n",
              " {'step': 990, 'train_loss': 4.395512580871582},\n",
              " {'step': 991, 'train_loss': 4.296439170837402},\n",
              " {'step': 992, 'train_loss': 4.393542289733887},\n",
              " {'step': 993, 'train_loss': 4.442497730255127},\n",
              " {'step': 994, 'train_loss': 4.592298984527588},\n",
              " {'step': 995, 'train_loss': 4.294090747833252},\n",
              " {'step': 996, 'train_loss': 4.3901872634887695},\n",
              " {'step': 997, 'train_loss': 4.449321746826172},\n",
              " {'step': 998, 'train_loss': 4.244108200073242},\n",
              " {'step': 999, 'train_loss': 4.285708904266357},\n",
              " {'step': 1000,\n",
              "  'train_loss': 4.281221866607666,\n",
              "  'valid_loss': 4.974805423191616},\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgO77jkVjDe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aea12818-1d59-4999-e18b-1b600d180c3c"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "print(output)\n",
        "output.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.5508, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}